{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4wl69UFybc_P"
   },
   "source": [
    "## **Preparing the work environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hfcIGKtybKci",
    "outputId": "a7daa3f6-8eb6-4649-9890-734f186ad138"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score,confusion_matrix,classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdJkoPZObvZu"
   },
   "source": [
    "## load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uq0jMP1Fbo1Q"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/train1.csv')\n",
    "test = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/test1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mRj-C4acaHO"
   },
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vn6YzeN_b0eg",
    "outputId": "b691d479-f424-4881-8864-c3f209a9a746"
   },
   "outputs": [],
   "source": [
    "print('Shape of train ',train.shape)\n",
    "print('Shape of test ',test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPjL2saDcA4j",
    "outputId": "53f0f490-3f62-405d-d048-1357b48940d8"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OpamiFQmcQFF",
    "outputId": "6655609d-4de9-46c9-caa1-27276ce859ff"
   },
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "NHR_KBF7cT_l",
    "outputId": "e9e07381-3b45-4c79-a4f9-fa66611d6c10"
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-3nY45cxcmSH"
   },
   "outputs": [],
   "source": [
    "target = train['target']\n",
    "train_id = train['id']\n",
    "test_id = test['id']\n",
    "train.drop(['id','date','user','flag'],axis=1,inplace=True)\n",
    "test.drop(['id','date','user','flag'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9NJic6CdJwc",
    "outputId": "1aedd1ba-11c3-4c71-c210-f1d7073d8e61"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QcFTSWeDdgN8",
    "outputId": "a9a3eca7-ec8e-4ae7-ba5d-cf701e7e2b55"
   },
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8i7gQEO5dt1-"
   },
   "source": [
    "## Preprocessing\n",
    "Extact text for column\n",
    "\n",
    "1- Remove punctuation: Punctuation marks such as commas, periods, and question marks do not provide much meaning for sentiment analysis, so you can remove them from the text.\n",
    "\n",
    "2- Convert text to lowercase: Convert all the text to lowercase so that similar words can be grouped together.\n",
    "\n",
    "3- Tokenize the text: Tokenization is the process of splitting a sentence into words. You can use NLTK library to tokenize the text.\n",
    "\n",
    "4- Remove stop words: Stop words are common words such as \"the\", \"and\", and \"in\" that do not add much meaning to the text. You can remove them using NLTK's stop words list.\n",
    "\n",
    "5- Stemming/Lemmatization: Stemming and lemmatization are techniques to reduce the word to its root form. It can help in reducing the feature space. You can use NLTK's WordNetLemmatizer for lemmatization or PorterStemmer for stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dI0GewqEdjvW"
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "  text = re.sub('[^a-zA-Z]',' ',text).strip()\n",
    "  text = text.lower()\n",
    "  tokens = word_tokenize(text)\n",
    "  filtered_tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
    "  stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "  return \" \".join(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KjRimrlOeBFV"
   },
   "outputs": [],
   "source": [
    "train['text'] = train['text'].apply(lambda x : preprocess_text(x))\n",
    "test['text'] = test['text'].apply(lambda x : preprocess_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxc4hxargj4B"
   },
   "source": [
    "### convert Data frame to numpy array and transform it to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qur5ha5txWAq"
   },
   "outputs": [],
   "source": [
    "X_train , X_val = train_test_split(train,test_size=.1,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMvsYZBHgtqa"
   },
   "outputs": [],
   "source": [
    "cv = TfidfVectorizer(ngram_range=(1,3), analyzer = 'word', use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
    "df_train = cv.fit_transform(X_train['text'])\n",
    "df_val = cv.transform(X_val['text'])\n",
    "df_test = cv.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ox7lGHwyTnH",
    "outputId": "b6f42220-17ad-4d14-8602-a572cfb7efc7"
   },
   "outputs": [],
   "source": [
    "print('df_train shape ',df_train.shape)\n",
    "print('df_train shape ',df_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I4AFwrkJsdYg"
   },
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=2,init='random',n_init=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 75
    },
    "id": "-JoGaSW5u25F",
    "outputId": "b483e6eb-a4a1-446e-f8ed-d5f9bdc598ff"
   },
   "outputs": [],
   "source": [
    "model.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MDoUJa--3sqR",
    "outputId": "8852702b-483f-4075-ec86-4b6277e24614"
   },
   "outputs": [],
   "source": [
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_NoaccZdu5Lc",
    "outputId": "e0aec654-9758-4086-ee46-d60207ad7990"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(df_val)\n",
    "conf_mat = confusion_matrix(X_val.target, pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "e6gTj170ItrG",
    "outputId": "7c38bc90-e5f2-419a-8d68-7aea68954dbf"
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(conf_mat/np.sum(conf_mat), annot=True,\n",
    "             fmt='.2%', cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "# ## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "# ## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3xvxLrsYvIzO",
    "outputId": "f32a0898-398c-4a5d-f023-39dbd5ebd787"
   },
   "outputs": [],
   "source": [
    "print(classification_report(X_val.target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_R7RXxltOAPH"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W9ipWDosM0xF",
    "outputId": "dbe01c60-a68e-4ad1-c9cb-060c2dfd47fb"
   },
   "outputs": [],
   "source": [
    "output = pd.DataFrame({'Id':test_id,\n",
    "                        'target': predictions})\n",
    "output.to_csv('kmeans2MMM.csv', index=False)\n",
    "\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2-t_ywJCykt"
   },
   "source": [
    "The message \"PCA does not support sparse input. See TruncatedSVD for a possible alternative\" means that the PCA algorithm in scikit-learn does not accept sparse input data.\n",
    "\n",
    "Sparse data refers to data that has a large number of zero values. In natural language processing, for example, sparse data is common when working with text data that has been converted to a bag-of-words representation.\n",
    "\n",
    "TruncatedSVD is an alternative algorithm that can be used for dimensionality reduction on sparse data. TruncatedSVD is similar to PCA, but specifically designed to handle sparse matrices as input. The TruncatedSVD algorithm is a variant of singular value decomposition (SVD) that can be used to reduce the dimensionality of sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELbD1Hpv4MO4"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "pca.fit(df_train)\n",
    "X_train_reduce = pca.transform(df_train)\n",
    "X_val_reduce = pca.transform(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1ZzHd5BKGy-"
   },
   "source": [
    "## Reduce the dimensionality using TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YuQFjKNu7E_g"
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=1000)\n",
    "X_train_reduce = svd.fit_transform(df_train)\n",
    "X_val_reduce = svd.fit_transform(df_val)\n",
    "test_reduce = svd.fit_transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCFuAgHcDoo7"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAepTrGiJ7N5"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_val_reduce)\n",
    "conf_mat = confusion_matrix(X_val.target, pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b0aGUBl1J-6I"
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(conf_mat/np.sum(conf_mat), annot=True,\n",
    "            fmt='.2%', cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Undic48KBV_"
   },
   "outputs": [],
   "source": [
    "print(classification_report(X_val.target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqmNNtqkMtBM"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_reduce)\n",
    "output = pd.DataFrame({'Id':test_id,\n",
    "                       'target': predictions})\n",
    "output.to_csv('kmeans2.csv', index=False)\n",
    "\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aogd5fP4KSUe"
   },
   "source": [
    "## Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BuMu1_0zLOna"
   },
   "source": [
    "Using the reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nIuf6kO4KkF0"
   },
   "outputs": [],
   "source": [
    "model2 = AgglomerativeClustering(n_clusters=2, linkage='ward')\n",
    "model2.fit(X_train_reduce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zaRWPfRLDQW"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(X_val_reduce)\n",
    "conf_mat = confusion_matrix(X_val.target, pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akxEBM1uLH4Q"
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(conf_mat/np.sum(conf_mat), annot=True,\n",
    "                 fmt='.2%', cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfId_5JrLKje"
   },
   "outputs": [],
   "source": [
    "print(classification_report(X_val.target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "008tOI5zMuHn"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(test_reduce)\n",
    "output = pd.DataFrame({'Id':test_id,\n",
    "                       'target': predictions})\n",
    "output.to_csv('kmeans3.csv', index=False)\n",
    "\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o9pPBTiWLSuO"
   },
   "source": [
    "using the data without reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI1EpjzVLXHJ"
   },
   "outputs": [],
   "source": [
    "model2.fit(df_train.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nl_kdHhaLaAS"
   },
   "outputs": [],
   "source": [
    "pred = model.predict(df_val)\n",
    "conf_mat = confusion_matrix(X_val.target, pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hHs3Qdq-LqT2"
   },
   "outputs": [],
   "source": [
    "ax = sns.heatmap(conf_mat/np.sum(conf_mat), annot=True,\n",
    "             fmt='.2%', cmap='Blues')\n",
    "\n",
    "ax.set_title('Seaborn Confusion Matrix with labels\\n\\n');\n",
    "ax.set_xlabel('\\nPredicted Values')\n",
    "ax.set_ylabel('Actual Values ');\n",
    "\n",
    "## Ticket labels - List must be in alphabetical order\n",
    "ax.xaxis.set_ticklabels(['False','True'])\n",
    "ax.yaxis.set_ticklabels(['False','True'])\n",
    "\n",
    "## Display the visualization of the Confusion Matrix.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O48RmNK5Lsre"
   },
   "outputs": [],
   "source": [
    "print(classification_report(X_val.target, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wy2E0PbfMvL3"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(df_test)\n",
    "output = pd.DataFrame({'Id':test_id,\n",
    "                      'target': predictions})\n",
    "output.to_csv('kmeans4.csv', index=False)\n",
    "\n",
    "print(\"Your submission was successfully saved!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
